
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Econometrics I</title><meta name="generator" content="MATLAB 9.9"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2022-02-18"><meta name="DC.source" content="s5.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; }

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }
span.typesection { color:#A0522D }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Econometrics I</h1><pre>TA Christian Alem&aacute;n</pre><p><b>Session 5: Friday 18, February 2022</b></p><p><b>Activity 1: Simulating Consistency of OLS</b></p><p>Consider the model</p><p><img src="s5_eq05998836314605936363.png" alt="$y_{i} = \beta_{0}+\beta_{1}x_{i}+\epsilon_{i}$"></p><p><img src="s5_eq07474211892256132982.png" alt="$x\sim U[a,b]$"></p><p>We will show consistency of <img src="s5_eq03568218979608940920.png" alt="$\beta_{1}^{OLS}$"></p><p><img src="s5_eq01080728704366400987.png" alt="$E(\hat{\beta}_{1}|X) = \beta_{1}$"></p><p>Lets assume different distributions of <img src="s5_eq04202927574850129020.png" alt="$\epsilon$"></p><div><ol><li>Normal : <img src="s5_eq10316733020292497255.png" alt="$\epsilon \sim \mathcal{N}(0,\sigma^{2})$"></li><li>Poisson: <img src="s5_eq04178333591362565020.png" alt="$\epsilon \sim Pois(\lambda)$"></li><li>Pareto:  <img src="s5_eq14671949257588866056.png" alt="$\epsilon \sim Pareto(x_{m},\alpha)$"></li></ol></div><p>We will Simulate a population of 50000</p><p>Make 10000 draws of sample of n = [25,100,1000,10000];</p><pre class="codeinput"><span class="comment">% Housekeeping</span>
clc
close <span class="string">all</span>
clear <span class="string">all</span>

<span class="comment">% Set seed for reproductibility</span>


<span class="comment">% Options</span>
opt.load_betas = 1;                 <span class="comment">% 1:load betas to save time</span>


<span class="comment">% Parameters</span>
par.N = 100000;                      <span class="comment">% Population</span>
par.n_ms = 10000;                    <span class="comment">% Number of MC simulations</span>
par.n_grid = [25,100,1000,10000];   <span class="comment">% Sample size</span>
par.beta0 = 1;
par.beta1 = 2;

<span class="comment">% Parameters distributions of \epsilon:</span>
par.sigma = 1;      <span class="comment">% Standard deviation normal</span>
par.lambda = 1;     <span class="comment">% Poisson Rate</span>
par.k = 1;          <span class="comment">% Shape if &gt;=1 var is infinity try 4</span>
par.sigma_gp = 2;   <span class="comment">% Scale</span>
par.theta = 0;
par.ub = 100;
par.lb = 0;

<span class="comment">% Generate population</span>
vars.e_pop = NaN(par.N,3);
<span class="keyword">for</span> i = 1:3
    <span class="keyword">if</span> i == 1
        rng(123+i)
        vars.e_pop(:,1) = par.sigma.*randn(par.N,1);
        vars.e_pop(:,1) = vars.e_pop(:,1)-mean(vars.e_pop(:,1)); <span class="comment">% demean</span>
    <span class="keyword">elseif</span> i == 2
        rng(123+i)
        vars.e_pop(:,2) = sim_poisson(par.N,par.lambda);
        vars.e_pop(:,2) = vars.e_pop(:,2)-mean(vars.e_pop(:,2));
    <span class="keyword">elseif</span> i == 3
        rng(123+i)
        vars.e_pop(:,3) = gprnd(par.k,par.sigma_gp,par.theta,par.N,1);
        <span class="comment">%vars.e_pop(:,3) = sim_gp(par.N,par.k,par.sigma_gp,par.theta);</span>
        vars.e_pop(:,3) = vars.e_pop(:,3)-mean(vars.e_pop(:,3));
    <span class="keyword">end</span>
<span class="keyword">end</span>

vars.x_pop = unifrnd(par.lb,par.ub,par.N,1);
vars.y_pop = par.beta0 +par.beta1.*repmat(vars.x_pop,1,3) +vars.e_pop;

<span class="comment">% Here we do the sampling and compute the distributions of parameters</span>

<span class="comment">% Initialize holder</span>
vars.hbeta1 = NaN(par.n_ms,4,3);

<span class="keyword">if</span> opt.load_betas ==1
    load(<span class="string">'data'</span>)
    vars.hbeta1 = mc_data;
<span class="keyword">else</span>
    <span class="comment">% Generate NS number of samples from the universe.</span>
    <span class="keyword">for</span> i = 1:3  <span class="comment">%  Distributions</span>
        <span class="keyword">for</span> ii = 1:4    <span class="comment">% Sample sizes</span>
            disp([i,ii])
            <span class="keyword">for</span> j = 1:par.n_ms   <span class="comment">% Number of MC simulations</span>

                I_sample = randsample(par.N,par.n_grid(ii));
                x_sample = vars.x_pop(I_sample);
                y_sample = vars.y_pop(I_sample,i);
                <span class="comment">% Run regression</span>
                X = [ones(par.n_grid(ii),1),x_sample];
                Y = y_sample;
                hat_betas = X\Y;
                <span class="comment">% Save betas</span>
                vars.hbeta1(j,ii,i) = hat_betas(2);
            <span class="keyword">end</span>
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    mc_data = vars.hbeta1;
    save(<span class="string">'data'</span>,<span class="string">'mc_data'</span>)
    <span class="comment">%save('data_inc','data_income')</span>
<span class="keyword">end</span>

vars.kdensity = NaN(100,4,3);            <span class="comment">% Initialize Kernel Density</span>
par.support  = NaN(100,4,3);
 <span class="keyword">for</span> i = 1:3  <span class="comment">%  Distributions</span>
        <span class="keyword">for</span> ii = 1:4    <span class="comment">% Sample sizes</span>
        <span class="comment">% Do kernel approximations to smooth histograms</span>
        [vars.kdensity(:,ii,i),par.support(:,ii,i)] = ksdensity(vars.hbeta1(:,ii,i),<span class="string">'Kernel'</span>,<span class="string">'normal'</span>);
        <span class="keyword">end</span>
 <span class="keyword">end</span>

figure(1)
hold <span class="string">on</span>
p1 = plot(par.support(:,1,1),vars.kdensity(:,1,1),<span class="string">'r-'</span>,<span class="string">'linewidth'</span>,1.2);
p2 = plot(par.support(:,2,1),vars.kdensity(:,2,1),<span class="string">'r--'</span>,<span class="string">'linewidth'</span>,1.2);
p3 = plot(par.support(:,3,1),vars.kdensity(:,3,1),<span class="string">'r-.'</span>,<span class="string">'linewidth'</span>,1.2);
p4 = plot(par.support(:,4,1),vars.kdensity(:,4,1),<span class="string">'r.'</span>,<span class="string">'linewidth'</span>,1.2);
xlim([1.98,2.02])
legend([p1 p2 p3 p4],{<span class="string">'$n=25$'</span>,<span class="string">'$n=100$'</span>,<span class="string">'$n=1000$'</span>,<span class="string">'$n=10000$'</span>},<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
title(<span class="string">'Kernel Density Approximations, Normal Errors'</span>)
ylabel(<span class="string">'Density'</span>)

figure(2)
hold <span class="string">on</span>
p1 = plot(par.support(:,1,2),vars.kdensity(:,1,2),<span class="string">'r-'</span>,<span class="string">'linewidth'</span>,1.2);
p2 = plot(par.support(:,2,2),vars.kdensity(:,2,2),<span class="string">'r--'</span>,<span class="string">'linewidth'</span>,1.2);
p3 = plot(par.support(:,3,2),vars.kdensity(:,3,2),<span class="string">'r-.'</span>,<span class="string">'linewidth'</span>,1.2);
p4 = plot(par.support(:,4,2),vars.kdensity(:,4,2),<span class="string">'r.'</span>,<span class="string">'linewidth'</span>,1.2);
xlim([1.98,2.02])
legend([p1 p2 p3 p4],{<span class="string">'$n=25$'</span>,<span class="string">'$n=100$'</span>,<span class="string">'$n=1000$'</span>,<span class="string">'$n=10000$'</span>},<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
title(<span class="string">'Kernel Density Approximations, Poisson Errors'</span>)
ylabel(<span class="string">'Density'</span>)

figure(3)
hold <span class="string">on</span>
p1 = plot(par.support(:,1,3),vars.kdensity(:,1,3),<span class="string">'r-'</span>,<span class="string">'linewidth'</span>,1.2);
p2 = plot(par.support(:,2,3),vars.kdensity(:,2,3),<span class="string">'r--'</span>,<span class="string">'linewidth'</span>,1.2);
p3 = plot(par.support(:,3,3),vars.kdensity(:,3,3),<span class="string">'r-.'</span>,<span class="string">'linewidth'</span>,1.2);
p4 = plot(par.support(:,4,3),vars.kdensity(:,4,3),<span class="string">'r.'</span>,<span class="string">'linewidth'</span>,1.2);
xlim([-4,10])
legend([p1 p2 p3 p4],{<span class="string">'$n=25$'</span>,<span class="string">'$n=100$'</span>,<span class="string">'$n=1000$'</span>,<span class="string">'$n=10000$'</span>},<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
title(<span class="string">'Kernel Density Approximations, Pareto Errors'</span>)
ylabel(<span class="string">'Density'</span>)

figure(4)
hold <span class="string">on</span>
p4 = plot(par.support(1:30,4,3),vars.kdensity(1:30,4,3),<span class="string">'r-'</span>,<span class="string">'linewidth'</span>,1.2);
legend([p4],{<span class="string">'$n=10000$'</span>},<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
title(<span class="string">'Kernel Density Approximations, Pareto Errors'</span>)
ylabel(<span class="string">'Density'</span>)
</pre><img vspace="5" hspace="5" src="s5_01.png" alt=""> <img vspace="5" hspace="5" src="s5_02.png" alt=""> <img vspace="5" hspace="5" src="s5_03.png" alt=""> <img vspace="5" hspace="5" src="s5_04.png" alt=""> <p><b>Activity 1.1: Asymtotic Normality</b></p><p>Normality test for the last case H0: x comes from a standard normal distribution</p><pre class="codeinput">name = {<span class="string">'Normal:'</span>,<span class="string">'Poisson:'</span>,<span class="string">'Pareto:'</span>};
<span class="keyword">for</span> i = 1:3
    disp(name{i})
    test_cdf = makedist(<span class="string">'normal'</span>,<span class="string">'mu'</span>,mean(vars.hbeta1(:,4,i)),<span class="string">'sigma'</span>,std(vars.hbeta1(:,4,i)));
    <span class="comment">%test_cdf = [vars.hbeta1(:,4,i),cdf('normal',vars.hbeta1(:,4,i),mean(vars.hbeta1(:,4,i)),std(vars.hbeta1(:,4,i)))];</span>
    h = kstest(vars.hbeta1(:,4,i),<span class="string">'CDF'</span>,test_cdf); <span class="comment">% Test if the data are from the hypothesized distribution.</span>
<span class="keyword">if</span> h==1
    disp(<span class="string">'We reject the Null: Then distribution is not normal'</span>)
<span class="keyword">else</span>
    disp(<span class="string">'We CANNOT reject the Null: Then distribution is normal'</span>)
<span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">Normal:
We CANNOT reject the Null: Then distribution is normal
Poisson:
We CANNOT reject the Null: Then distribution is normal
Pareto:
We reject the Null: Then distribution is not normal
</pre><p><b>Activity 2: Testing Non Linear Constrains WALD test</b></p><p>Consider the following consumption function with different short- and long-run marginal propensities to consume (MPC).</p><p><img src="s5_eq05320808419870489639.png" alt="$lnC_{t}=\beta_{1}+\beta_{2}lnY_{t}+\beta_{3}lnC_{t-1}+\epsilon_{t}$"></p><div><ol><li><img src="s5_eq09291292063872606237.png" alt="$C_{t}$"> is Consumption at <img src="s5_eq12656067504604335951.png" alt="$t$"> (in real USD)</li><li><img src="s5_eq11931547922297436405.png" alt="$Y_{t}$"> is Disposable Income at <img src="s5_eq12656067504604335951.png" alt="$t$"> (in real USD)</li></ol></div><div><ol><li><img src="s5_eq13747041898813073110.png" alt="$\beta_{2}: Short Run MPC$"></li><li><img src="s5_eq01510484006678018294.png" alt="$\frac{\beta_{2}}{1-\beta_{3}}=\gamma: Long Run MPC$"></li></ol></div><p>We are interested in knowing whether <img src="s5_eq10510864866069374395.png" alt="$\gamma=1$"></p><pre class="codeinput">load(<span class="string">'data_inc2'</span>)
<span class="comment">%{
</span><span class="comment">1: C0
</span><span class="comment">2: C1
</span><span class="comment">3: Y
</span><span class="comment">%}
</span>data_income = log(data_income);
n = size(data_income,1);
K = 3;
X = [ones(n,1),data_income(:,3),data_income(:,2)];
y = data_income(:,1);
<span class="comment">% Compute OLS</span>
betas = X\y;
hgamma = betas(2)/(1-betas(3));

Param = {<span class="string">'beta 1'</span>;<span class="string">'beta 2'</span>;<span class="string">'beta 3'</span>;<span class="string">'gamma'</span>};


hat_betas = [betas;hgamma];
tab_B = table(Param,hat_betas);
disp(<span class="string">'OLS Estimates:'</span>)
disp(tab_B)
</pre><pre class="codeoutput">OLS Estimates:
      Param       hat_betas
    __________    _________

    {'beta 1'}    0.0031416
    {'beta 2'}     0.074958
    {'beta 3'}      0.92462
    {'gamma' }      0.99446

</pre><p>Predict errors</p><pre class="codeinput">he = y-X*betas;
hsigma = (he'*he)/(n-K);
<span class="comment">% Compute Asymtotic VarCovar</span>
Avar_beta = hsigma.*(inv(X'*X));
AVCV = sqrt((diag(Avar_beta)));     <span class="comment">% Variance Covariance</span>

<span class="comment">% Compute the analytical derivatives:</span>
<span class="comment">% Derivative: \partial beta2/(1-beta3)\partial(\beta)</span>
grad = [0;1/(1-betas(3));betas(2)/(1-betas(3))^(2)];
<span class="comment">% Compute the asymtotic Variance</span>
Avar_Cbeta = grad'*Avar_beta*grad;
<span class="comment">% Since we are testing one restriction we compute the z-score:</span>
<span class="comment">% H0: gamma = 1</span>
z = (hgamma-1)./sqrt(Avar_Cbeta);
disp([<span class="string">'Z-Score = '</span>,num2str(z)])
disp(<span class="string">'We cannot reject \gamma=1'</span>)
</pre><pre class="codeoutput">Z-Score = -0.33863
We cannot reject \gamma=1
</pre><p><b>Activity 3: Bootstrap standard errors</b></p><p><b>3.1 Non-Parametric Bootstrap</b></p><div><ol><li>Generate B samples with replacement of pairs <img src="s5_eq04490705017860987126.png" alt="$(y{i},x_{i})$"></li><li>Estimate the bootstrap <img src="s5_eq07299505863887557152.png" alt="$\hat{\beta}$"> by fitting the model</li><li>Compute the standard errors</li></ol></div><pre class="codeinput">par.n = par.n_grid(3);      <span class="comment">% Sample size</span>
par.K = 2;
vars.x =1+2*randn(par.n,2) ;
vars.x(:,1) =4+vars.x(:,1);
vars.e =2*randn(par.n,1) ;
vars.y = vars.x*ones(2 ,1) + vars.e ;
<span class="keyword">if</span> par.K ==3
    vars.x = [ones(par.n,1),vars.x];
    vars.y = par.beta0+vars.x(:,2:end)*ones(2 ,1) + vars.e ;
<span class="keyword">end</span>



vars.hbeta = vars.x\vars.y ;
vars.ehat = vars.y-vars.x*vars.hbeta;
par.B = 1000;<span class="comment">%</span>

hbeta_sample =NaN(par.B,par.K);
<span class="keyword">for</span> i = 1:par.B

    I_sample = ceil(par.n*rand(1,par.n));
    ysample = vars.y(I_sample);
    xsample(:,1) = vars.x(I_sample,1);
    xsample(:,2) = vars.x(I_sample,2);

    hbeta_sample(i,:)  = (xsample\ysample)';
<span class="keyword">end</span>
diff = hbeta_sample-repmat(vars.hbeta',par.B,1);
bootVCV = diff'*diff/par.B;
OLSVCV = (vars.e'*vars.e)/par.n*inv(vars.x'*vars.x);

vars.SEbeta_hat_OLS = sqrt(diag(OLSVCV));
vars.SEbeta_hat_boot = sqrt(diag(bootVCV));

Param = {<span class="string">'beta 0'</span>;<span class="string">'beta 1'</span>};
<span class="keyword">if</span>  par.K ==3
    Param = {<span class="string">'beta 0'</span>;<span class="string">'beta 1'</span>,<span class="string">'beta2'</span>};
<span class="keyword">end</span>

SE_OLS = vars.SEbeta_hat_OLS;
SE_Bootstrap = vars.SEbeta_hat_boot;
tab_SE = table(Param,SE_OLS,SE_Bootstrap);
disp(<span class="string">'Non-Parametric Standard Errors:'</span>)
disp(tab_SE)
</pre><pre class="codeoutput">Non-Parametric Standard Errors:
      Param        SE_OLS     SE_Bootstrap
    __________    ________    ____________

    {'beta 0'}    0.012984      0.012932  
    {'beta 1'}    0.031073      0.030937  

</pre><p><b>3.2 Parametric Bootstrap</b></p><div><ol><li>Generate 2*B samples with replacement of <img src="s5_eq08380032906119107581.png" alt="$e_{i},x_{i}$"> indepedently</li><li>Construct values of <img src="s5_eq08830444604280721118.png" alt="$y$"></li><li>Estimate the bootstrap <img src="s5_eq07299505863887557152.png" alt="$\hat{\beta}$"> by fitting the model</li><li>Compute the standard errors</li></ol></div><pre class="codeinput"><span class="keyword">for</span> i = 1:par.B

    I_sample = ceil(par.n*rand(1,par.n));
    II_sample = ceil(par.n*rand(1,par.n));
    esample = vars.ehat(I_sample);
    xsample(:,1) = vars.x(I_sample,1);
    xsample(:,2) = vars.x(I_sample,2);
    ysample = xsample*vars.hbeta+esample;
    hbeta_sample(i,:)  = (xsample\ysample)';
<span class="keyword">end</span>
diff = hbeta_sample-repmat(vars.hbeta',par.B,1);
bootVCV = diff'*diff/par.B;
OLSVCV = (vars.e'*vars.e)/par.n*inv(vars.x'*vars.x);

vars.SEbeta_hat_OLS = sqrt(diag(OLSVCV));
vars.SEbeta_hat_boot = sqrt(diag(bootVCV));


SE_OLS = vars.SEbeta_hat_OLS;
SE_Bootstrap = vars.SEbeta_hat_boot;
tab_SE = table(Param,SE_OLS,SE_Bootstrap);
disp(<span class="string">'Parametric Standard Errors:'</span>)
disp(tab_SE)


dert_stop = 1;
<span class="comment">%-----------------------------------------------------------</span>
<span class="keyword">function</span> data = sim_gp(N,k,sigma,theta)
<span class="comment">% This function simulates a vector data for the generalized Pareto using the inverse</span>
<span class="comment">% CDF method:</span>
<span class="comment">%{
</span><span class="comment">
</span><span class="comment">Inputs:
</span><span class="comment">    N     : Number of observations to be simulated
</span><span class="comment">    k     : Index Shape \chi
</span><span class="comment">    sigma : Scale
</span><span class="comment">    theta : Threshold location \mu
</span><span class="comment">
</span><span class="comment">Output:
</span><span class="comment">    data  : NX1 vector of data
</span><span class="comment">
</span><span class="comment">With shape &#958; &gt; 0  and location &#956; = &#963; / &#958;
</span><span class="comment">the GPD is equivalent to the Pareto distribution with scale x m = &#963; / &#958;
</span><span class="comment"> and shape &#945; = 1 / &#958;
</span><span class="comment">%}
</span>var_x = rand(N,1);
data = sigma./k.*((1-var_x).^(-k)-1)+theta;
<span class="keyword">end</span>
<span class="comment">%-----------------------------------------------------------</span>
<span class="keyword">function</span> data = sim_poisson(N,lambda)
<span class="comment">% This function simulates a vector data for the generalized Poisson Distribution using the inverse</span>
<span class="comment">% CDF method:</span>
<span class="comment">%{
</span><span class="comment">
</span><span class="comment">Inputs:
</span><span class="comment">    N          : Number of observations to be simulated
</span><span class="comment">    lambda     : Poisson rate
</span><span class="comment">
</span><span class="comment">Output:
</span><span class="comment">    data  : NX1 vector of data
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%}
</span>var_x = rand(N,1);
data = icdf(<span class="string">'Poisson'</span>,var_x,lambda);
<span class="keyword">end</span>
</pre><pre class="codeoutput">Parametric Standard Errors:
      Param        SE_OLS     SE_Bootstrap
    __________    ________    ____________

    {'beta 0'}    0.012984       0.01324  
    {'beta 1'}    0.031073      0.030953  

</pre><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2020b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Econometrics I 
%  TA Christian Alemán
% 
% *Session 5: Friday 18, February 2022*
%
% *Activity 1: Simulating Consistency of OLS* 
%
% Consider the model
%
% $y_{i} = \beta_{0}+\beta_{1}x_{i}+\epsilon_{i}$
%
% $x\sim U[a,b]$
% 
% We will show consistency of $\beta_{1}^{OLS}$
%
% $E(\hat{\beta}_{1}|X) = \beta_{1}$
%
% Lets assume different distributions of $\epsilon$
% 
% # Normal : $\epsilon \sim \mathcal{N}(0,\sigma^{2})$
% # Poisson: $\epsilon \sim Pois(\lambda)$
% # Pareto:  $\epsilon \sim Pareto(x_{m},\alpha)$
%
% We will Simulate a population of 50000
% 
% Make 10000 draws of sample of n = [25,100,1000,10000];
%


% Housekeeping
clc
close all
clear all

% Set seed for reproductibility


% Options
opt.load_betas = 1;                 % 1:load betas to save time


% Parameters
par.N = 100000;                      % Population
par.n_ms = 10000;                    % Number of MC simulations
par.n_grid = [25,100,1000,10000];   % Sample size
par.beta0 = 1;
par.beta1 = 2;

% Parameters distributions of \epsilon:
par.sigma = 1;      % Standard deviation normal
par.lambda = 1;     % Poisson Rate
par.k = 1;          % Shape if >=1 var is infinity try 4
par.sigma_gp = 2;   % Scale
par.theta = 0;
par.ub = 100;
par.lb = 0; 

% Generate population 
vars.e_pop = NaN(par.N,3);
for i = 1:3
    if i == 1
        rng(123+i)
        vars.e_pop(:,1) = par.sigma.*randn(par.N,1);
        vars.e_pop(:,1) = vars.e_pop(:,1)-mean(vars.e_pop(:,1)); % demean
    elseif i == 2
        rng(123+i)
        vars.e_pop(:,2) = sim_poisson(par.N,par.lambda);
        vars.e_pop(:,2) = vars.e_pop(:,2)-mean(vars.e_pop(:,2));
    elseif i == 3
        rng(123+i)
        vars.e_pop(:,3) = gprnd(par.k,par.sigma_gp,par.theta,par.N,1);
        %vars.e_pop(:,3) = sim_gp(par.N,par.k,par.sigma_gp,par.theta);
        vars.e_pop(:,3) = vars.e_pop(:,3)-mean(vars.e_pop(:,3));
    end
end

vars.x_pop = unifrnd(par.lb,par.ub,par.N,1);
vars.y_pop = par.beta0 +par.beta1.*repmat(vars.x_pop,1,3) +vars.e_pop;

% Here we do the sampling and compute the distributions of parameters

% Initialize holder
vars.hbeta1 = NaN(par.n_ms,4,3);

if opt.load_betas ==1
    load('data')
    vars.hbeta1 = mc_data;
else
    % Generate NS number of samples from the universe.
    for i = 1:3  %  Distributions
        for ii = 1:4    % Sample sizes
            disp([i,ii])
            for j = 1:par.n_ms   % Number of MC simulations
                
                I_sample = randsample(par.N,par.n_grid(ii)); 
                x_sample = vars.x_pop(I_sample);
                y_sample = vars.y_pop(I_sample,i);
                % Run regression
                X = [ones(par.n_grid(ii),1),x_sample];
                Y = y_sample;
                hat_betas = X\Y;
                % Save betas
                vars.hbeta1(j,ii,i) = hat_betas(2);
            end
        end
    end

    mc_data = vars.hbeta1;
    save('data','mc_data')
    %save('data_inc','data_income')
end

vars.kdensity = NaN(100,4,3);            % Initialize Kernel Density
par.support  = NaN(100,4,3);     
 for i = 1:3  %  Distributions
        for ii = 1:4    % Sample sizes
        % Do kernel approximations to smooth histograms      
        [vars.kdensity(:,ii,i),par.support(:,ii,i)] = ksdensity(vars.hbeta1(:,ii,i),'Kernel','normal');
        end
 end

figure(1)
hold on
p1 = plot(par.support(:,1,1),vars.kdensity(:,1,1),'r-','linewidth',1.2);
p2 = plot(par.support(:,2,1),vars.kdensity(:,2,1),'rREPLACE_WITH_DASH_DASH','linewidth',1.2);
p3 = plot(par.support(:,3,1),vars.kdensity(:,3,1),'r-.','linewidth',1.2);
p4 = plot(par.support(:,4,1),vars.kdensity(:,4,1),'r.','linewidth',1.2);
xlim([1.98,2.02])
legend([p1 p2 p3 p4],{'$n=25$','$n=100$','$n=1000$','$n=10000$'},'interpreter','latex')
title('Kernel Density Approximations, Normal Errors')
ylabel('Density')

figure(2)
hold on
p1 = plot(par.support(:,1,2),vars.kdensity(:,1,2),'r-','linewidth',1.2);
p2 = plot(par.support(:,2,2),vars.kdensity(:,2,2),'rREPLACE_WITH_DASH_DASH','linewidth',1.2);
p3 = plot(par.support(:,3,2),vars.kdensity(:,3,2),'r-.','linewidth',1.2);
p4 = plot(par.support(:,4,2),vars.kdensity(:,4,2),'r.','linewidth',1.2);
xlim([1.98,2.02])
legend([p1 p2 p3 p4],{'$n=25$','$n=100$','$n=1000$','$n=10000$'},'interpreter','latex')
title('Kernel Density Approximations, Poisson Errors')
ylabel('Density')

figure(3)
hold on
p1 = plot(par.support(:,1,3),vars.kdensity(:,1,3),'r-','linewidth',1.2);
p2 = plot(par.support(:,2,3),vars.kdensity(:,2,3),'rREPLACE_WITH_DASH_DASH','linewidth',1.2);
p3 = plot(par.support(:,3,3),vars.kdensity(:,3,3),'r-.','linewidth',1.2);
p4 = plot(par.support(:,4,3),vars.kdensity(:,4,3),'r.','linewidth',1.2);
xlim([-4,10])
legend([p1 p2 p3 p4],{'$n=25$','$n=100$','$n=1000$','$n=10000$'},'interpreter','latex')
title('Kernel Density Approximations, Pareto Errors')
ylabel('Density')

figure(4)
hold on
p4 = plot(par.support(1:30,4,3),vars.kdensity(1:30,4,3),'r-','linewidth',1.2);
legend([p4],{'$n=10000$'},'interpreter','latex')
title('Kernel Density Approximations, Pareto Errors')
ylabel('Density')


%%
% *Activity 1.1: Asymtotic Normality* 
%
% Normality test for the last case
% H0: x comes from a standard normal distribution
name = {'Normal:','Poisson:','Pareto:'};
for i = 1:3
    disp(name{i})
    test_cdf = makedist('normal','mu',mean(vars.hbeta1(:,4,i)),'sigma',std(vars.hbeta1(:,4,i)));
    %test_cdf = [vars.hbeta1(:,4,i),cdf('normal',vars.hbeta1(:,4,i),mean(vars.hbeta1(:,4,i)),std(vars.hbeta1(:,4,i)))];
    h = kstest(vars.hbeta1(:,4,i),'CDF',test_cdf); % Test if the data are from the hypothesized distribution.
if h==1
    disp('We reject the Null: Then distribution is not normal')
else
    disp('We CANNOT reject the Null: Then distribution is normal')
end
end


%%
% *Activity 2: Testing Non Linear Constrains WALD test*
%
% Consider the following consumption function with different short- and
% long-run marginal propensities to consume (MPC).
%
% $lnC_{t}=\beta_{1}+\beta_{2}lnY_{t}+\beta_{3}lnC_{t-1}+\epsilon_{t}$
%
% # $C_{t}$ is Consumption at $t$ (in real USD)
% # $Y_{t}$ is Disposable Income at $t$ (in real USD)
%
% # $\beta_{2}: Short Run MPC$
% # $\frac{\beta_{2}}{1-\beta_{3}}=\gamma: Long Run MPC$
% 
% We are interested in knowing whether $\gamma=1$
load('data_inc2')
%{
1: C0
2: C1
3: Y
%}
data_income = log(data_income);
n = size(data_income,1);
K = 3; 
X = [ones(n,1),data_income(:,3),data_income(:,2)];
y = data_income(:,1);
% Compute OLS
betas = X\y;
hgamma = betas(2)/(1-betas(3));

Param = {'beta 1';'beta 2';'beta 3';'gamma'};


hat_betas = [betas;hgamma];
tab_B = table(Param,hat_betas);
disp('OLS Estimates:')
disp(tab_B)

%%
% Predict errors
he = y-X*betas;
hsigma = (he'*he)/(n-K);
% Compute Asymtotic VarCovar
Avar_beta = hsigma.*(inv(X'*X));
AVCV = sqrt((diag(Avar_beta)));     % Variance Covariance

% Compute the analytical derivatives:
% Derivative: \partial beta2/(1-beta3)\partial(\beta)
grad = [0;1/(1-betas(3));betas(2)/(1-betas(3))^(2)];
% Compute the asymtotic Variance
Avar_Cbeta = grad'*Avar_beta*grad;
% Since we are testing one restriction we compute the z-score:
% H0: gamma = 1
z = (hgamma-1)./sqrt(Avar_Cbeta);
disp(['Z-Score = ',num2str(z)])
disp('We cannot reject \gamma=1')
%%
% *Activity 3: Bootstrap standard errors*
%
% *3.1 Non-Parametric Bootstrap*
%
% # Generate B samples with replacement of pairs $(y{i},x_{i})$
% # Estimate the bootstrap $\hat{\beta}$ by fitting the model
% # Compute the standard errors

par.n = par.n_grid(3);      % Sample size
par.K = 2;
vars.x =1+2*randn(par.n,2) ;
vars.x(:,1) =4+vars.x(:,1);
vars.e =2*randn(par.n,1) ;
vars.y = vars.x*ones(2 ,1) + vars.e ;
if par.K ==3
    vars.x = [ones(par.n,1),vars.x];
    vars.y = par.beta0+vars.x(:,2:end)*ones(2 ,1) + vars.e ;
end



vars.hbeta = vars.x\vars.y ;  
vars.ehat = vars.y-vars.x*vars.hbeta;
par.B = 1000;%

hbeta_sample =NaN(par.B,par.K);
for i = 1:par.B   
    
    I_sample = ceil(par.n*rand(1,par.n));
    ysample = vars.y(I_sample);
    xsample(:,1) = vars.x(I_sample,1);
    xsample(:,2) = vars.x(I_sample,2);
    
    hbeta_sample(i,:)  = (xsample\ysample)';
end
diff = hbeta_sample-repmat(vars.hbeta',par.B,1);
bootVCV = diff'*diff/par.B;
OLSVCV = (vars.e'*vars.e)/par.n*inv(vars.x'*vars.x);

vars.SEbeta_hat_OLS = sqrt(diag(OLSVCV));
vars.SEbeta_hat_boot = sqrt(diag(bootVCV));

Param = {'beta 0';'beta 1'};
if  par.K ==3
    Param = {'beta 0';'beta 1','beta2'};
end

SE_OLS = vars.SEbeta_hat_OLS;
SE_Bootstrap = vars.SEbeta_hat_boot;
tab_SE = table(Param,SE_OLS,SE_Bootstrap);
disp('Non-Parametric Standard Errors:')
disp(tab_SE)

%%
% *3.2 Parametric Bootstrap*
%
% # Generate 2*B samples with replacement of $e_{i},x_{i}$ indepedently
% # Construct values of $y$
% # Estimate the bootstrap $\hat{\beta}$ by fitting the model
% # Compute the standard errors


for i = 1:par.B   
    
    I_sample = ceil(par.n*rand(1,par.n));
    II_sample = ceil(par.n*rand(1,par.n));
    esample = vars.ehat(I_sample);
    xsample(:,1) = vars.x(I_sample,1);
    xsample(:,2) = vars.x(I_sample,2);
    ysample = xsample*vars.hbeta+esample;
    hbeta_sample(i,:)  = (xsample\ysample)';
end
diff = hbeta_sample-repmat(vars.hbeta',par.B,1);
bootVCV = diff'*diff/par.B;
OLSVCV = (vars.e'*vars.e)/par.n*inv(vars.x'*vars.x);

vars.SEbeta_hat_OLS = sqrt(diag(OLSVCV));
vars.SEbeta_hat_boot = sqrt(diag(bootVCV));


SE_OLS = vars.SEbeta_hat_OLS;
SE_Bootstrap = vars.SEbeta_hat_boot;
tab_SE = table(Param,SE_OLS,SE_Bootstrap);
disp('Parametric Standard Errors:')
disp(tab_SE)


dert_stop = 1;
%REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
function data = sim_gp(N,k,sigma,theta)
% This function simulates a vector data for the generalized Pareto using the inverse
% CDF method:
%{

Inputs:
    N     : Number of observations to be simulated
    k     : Index Shape \chi
    sigma : Scale
    theta : Threshold location \mu

Output:
    data  : NX1 vector of data

With shape ξ > 0  and location μ = σ / ξ 
the GPD is equivalent to the Pareto distribution with scale x m = σ / ξ
 and shape α = 1 / ξ 
%}
var_x = rand(N,1);
data = sigma./k.*((1-var_x).^(-k)-1)+theta;
end
%REPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASHREPLACE_WITH_DASH_DASH-
function data = sim_poisson(N,lambda)
% This function simulates a vector data for the generalized Poisson Distribution using the inverse
% CDF method:
%{

Inputs:
    N          : Number of observations to be simulated
    lambda     : Poisson rate

Output:
    data  : NX1 vector of data


%}
var_x = rand(N,1);
data = icdf('Poisson',var_x,lambda);
end

##### SOURCE END #####
--></body></html>